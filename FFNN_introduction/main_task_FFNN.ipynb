{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAbS69NPtdTJ"
      },
      "source": [
        "# Introduction to feed-forward neural networks\n",
        "\n",
        "---\n",
        "\n",
        "Lecture: \"Physics-augmented machine learning\" @ Cyber-Physical Simulation, TU Darmstadt\n",
        "\n",
        "Lecturer: Prof. Oliver Weeger\n",
        "\n",
        "Assistants: Dr.-Ing. Maximilian Kannapin, Jasper O. Schommartz, Dominik K. Klein\n",
        "\n",
        "Summer term 2024\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### In this notebook, you will...\n",
        "\n",
        "\n",
        "*   Calibrate feed-forward neural networks to different one-dimensional datasets\n",
        "*   Learn the influence of hyperparameters on the calibrated model\n",
        "* Learn the difference between interpolation and extrapolation\n",
        "*   Learn to construct convex and monotonous neural networks\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xin1T39xsu0"
      },
      "source": [
        "*Run the following cell to clone the GitHub repository in your current Google Colab environment.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_-FEL0BxvLk",
        "outputId": "8e0767a7-6336-4b5d-a6d6-a220fff77a1b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/CPShub/LecturePhysicsAwareML.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-v5FNcbWBpi"
      },
      "source": [
        "*Run the following cell to import all modules and python files to this notebook. If you made changes in the python files, run the following cell again to update the python files in this notebook. You might need to restart your Colab session first (\"Runtime / Restart session\" in the header menu).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFy4zsH0WAz6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "now = datetime.datetime.now\n",
        "import LecturePhysicsAwareML.FFNN_introduction.data as ld\n",
        "import LecturePhysicsAwareML.FFNN_introduction.models as lm\n",
        "import LecturePhysicsAwareML.FFNN_introduction.plots as lp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Run this cell if you are executing the notebook locally on your device.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "now = datetime.datetime.now\n",
        "import data as ld\n",
        "import models as lm\n",
        "import plots as lp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8QQsUEEvTxE"
      },
      "source": [
        "*If you want to clone the repository again, you have to delete it from your Google Colab files first. For this, you can run the following cell.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nOi7jY3vbsn"
      },
      "outputs": [],
      "source": [
        "%rm -rf LecturePhysicsAwareML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skD3hX3uxA5b"
      },
      "source": [
        "## Task 1: Nonlinear regression\n",
        "\n",
        "\n",
        "''In a nutshell, feed-forward neural networks (FFNNs) can be seen as a composition of several vector-valued functions, where the components of the vectors are referred to as nodes or neurons, and the function in each neuron is referred to as activation function. More explicitely, the FFNN with the vector-valued inpout $\\boldsymbol{x}=:\\boldsymbol{A}_0\\in\\mathbb{R}^{n^{[0]}}$, output $\\boldsymbol{y}=:\\boldsymbol{A}_{[H+1]}\\in\\mathbb{R}^{n^{[H+1]}}$, and $H$ hidden layers is given by\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{A}_h=\\sigma^{[h]}\\left(\\boldsymbol{w}^T\\boldsymbol{x}+\\boldsymbol{b}\\right)\\in\\mathbb{R}^{n^{[h]}}\\,,\\qquad h=1,...,H+1\\,.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Here, $\\boldsymbol{w}^{[h]}\\in\\mathbb{R}^{n^{[h]}\\times n^{[h-1]}}$ are the weights and $\\boldsymbol{b}\\in\\mathbb{R}^{n^{[h]}}$ the bias in each layer. Together, they form the set of parameters $\\boldsymbol{\\phi}$ of the neural network, which is optimized in the calibration process to fit a given dataset. In the layers $\\boldsymbol{A}_h$, the scalar activation functions $\\sigma^{[h]}$ are applied in a component-wise manner.'' (Klein et al., CMAME 400:115501) A FFNN has different hyperparameters. Here, the hyperparameters are given by the number of nodes and layers and the activation functions.\n",
        "\n",
        "<br>\n",
        "\n",
        "After fixing the FFNNs hyperparameters, it can be calibrated to fit a given dataset. In this notebook, we consider datasets of the form\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{D}=\\big\\{(x_i,\\,y_i) \\big\\}_{i=1,...,D}\\,,\\qquad x_i\\in\\mathbb{R},\\,y_i\\in\\mathbb{R}\\,.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Consequently, input and output of the FFNNs are also scalar-valued, i.e., $\\mathbb{R}^{n^{[0]}}=\\mathbb{R}^{n^{[H+1]}}=1$. The parameters $\\boldsymbol{\\phi}$ of the FFNN are determined such that it best-approximates the data $\\mathcal{D}$. Typically, this is done by minimizing a loss (or cost) function which provides a measure for the difference between data and FFNN model prediction. Here, the mean-squared error (MSE) is applied:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\operatorname{MSE}(\\boldsymbol{\\phi},\\,\\mathcal{D})=\\frac{1}{D}\\sum_{d=1}^D\\lvert\\lvert \\operatorname{FFNN}(x_i,\\,\\boldsymbol{\\phi})-y_i\\rvert\\rvert_2^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Since this is a highly nonlinear minimisation problem, iterative solution methods are applied. The number of iterations (or epochs) in the optimization process influences the quality of the calibrated model.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Tasks\n",
        "\n",
        "Despite their simple structure, FFNNs can represent basically every continuous function. Here, this is demonstrated by calibrating different FFNNs to different datasets. Furthermore, the influence of model and calibration hyperparameters are investigated.\n",
        "\n",
        "\n",
        "*  Calibrate FFNNs to different datasets ('bathtub', 'curve', 'double_curve')\n",
        "* Vary the number of hidden layers in [1, 2, 3] and the number of nodes in [4, 8, 16].\n",
        "* Vary the number of epochs in the parameter optimization process in [500, 1000, 2500, 3000].\n",
        "* Use different activation functions, e.g., Relu, Softplus and Sigmoid. Use a linear activation function in the output layer.\n",
        "\n",
        "Note that you do not have to evaluate all possible combinations, but only enough to understand the influence of the different hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "19DfAdMzYFeS",
        "outputId": "84cc427b-d242-4708-9415-9dd5333816cb"
      },
      "outputs": [],
      "source": [
        "# Adapt the model name for your plots\n",
        "model_name = 'FFNN'\n",
        "\n",
        "# Number of nodes in each layer\n",
        "units = [32,32,1]\n",
        "\n",
        "# Activation function in each layer\n",
        "# Options: 'softplus', 'tanh', 'relu', 'linear', ...\n",
        "activation = ['softplus','softplus','linear']\n",
        "\n",
        "# Load model\n",
        "model = lm.main(units=units, activation=activation)\n",
        "\n",
        "# Dataset options: 'bathtub', 'curve', 'double_curve'\n",
        "data = 'curve'\n",
        "\n",
        "# Epochs: number of iterations in the optimisation process\n",
        "epochs = 1000\n",
        "\n",
        "# Load data\n",
        "xs, ys, xs_c, ys_c = ld.get_data(data)\n",
        "\n",
        "# Calibrate model\n",
        "t1 = now()\n",
        "print(t1)\n",
        "\n",
        "# set \"verbose=2\" to observe the progress of the calibration process\n",
        "model.optimizer.learning_rate.assign(0.002)\n",
        "h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 0)\n",
        "\n",
        "t2 = now()\n",
        "print('it took', t2 - t1, '(sec) to calibrate the model')\n",
        "\n",
        "# Plot loss and prediction\n",
        "lp.plot_loss(h)\n",
        "lp.plot_data_model(xs, ys, xs_c, ys_c, model, model_name, data, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li53gWrYxJwE"
      },
      "source": [
        "## Task 2: Convex and monotonous neural networks\n",
        "\n",
        "''To lay the foundational intuition for constructing convex and monotonus neural networks, we first consider the univariate function\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "f:\\mathbb{R}\\rightarrow\\mathbb{R},\\quad x\\mapsto f(x):=(g\\circ h)(x)=g(h(x))\\,,\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "where $f$ is composed of two functions $g,h:\\mathbb{R}\\rightarrow \\mathbb{R}$. Given that all of these functions are twice continuously differentiable, convexity of $f$ in $x$ is equivalent to the nonnegativity of the second derivative\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "f''(x)=(g''\\circ h)(x)\\, h'(x)^2 + (g' \\circ h)(x)\\, h''(x) \\geq 0\\,.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "A sufficient, albeit not necessary condition for this is that the function $h$ is convex ($h''\\geq 0$), while the function $g$ is convex and nondecreasing ($g'\\geq 0,\\,g''\\geq 0$). Conversely, if a function acting on a convex function does not fulfill these conditions, the resulting function is not necessarily convex. The recursive application of above equation yields conditions for arbitrary many function compositions. The innermost function, here $h$, must only be convex, while every followung function must be convex and nondecreasing to preserve conxevity.\n",
        "\n",
        "The generalization to compositions of multivariate functions is also straightforward. For this, we consider the function\n",
        "\n",
        "\\begin{equation}\n",
        "f:\\mathbb{R}^m\\rightarrow\\mathbb{R},\\qquad\\boldsymbol{x}\\mapsto f(\\boldsymbol{x}):=(g\\circ\\boldsymbol{h})(\\boldsymbol{x})\\,,\n",
        "\\end{equation}\n",
        "\n",
        "with $\\boldsymbol{h}:\\mathbb{R}^m\\rightarrow\\mathbb{R}^n$ and $g:\\mathbb{R}^n\\rightarrow\\mathbb{R}$. Given that all of these functions are twice continuously differentiable, convexity of $f$ in $\\mathbb{x}$ is equivalent to the positive semi-definiteness of its Hessian. Similar reasoning as above leads to the sufficient condition that $\\boldsymbol{h}$ must be component-wise convex, while $g$ must be convex and nondecreasing, see (Klein et al, JMPS 159:104703) for an explicit proof. Again, the recursive application of this yields conditions for arbitrary many function compositions. Here, the innermost function must be component-wise convex, while every following function must be component-wise convex and nondecreasing to preserve convexity.\n",
        "\n",
        "In the same manner, the composite function $f$ is monotonically increasing (or nondecreasing) when its first derivative\n",
        "\n",
        "\\begin{equation}\n",
        "f'(x)=(g'\\circ h)(x)h'(x)\\geq 0\n",
        "\\end{equation}\n",
        "\n",
        "is nonnegative, which is fulfilled when both $g$ and $h$ are nondecreasing functions ($g'\\geq 0,\\,h'\\geq0$). The recursive application of this yields again conditions for arbitrary many function compositions. When all functions within a composite function are nondecreasing, the overall function is nondecreasing. In this case, the generalization to compositions of vector-valued functions leads to the condition that all functions must be component-wise nondecreasing.\n",
        " '' (Klein et al., DCE 4:e25)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "To summarize, sufficient conditions for convex neural networks are\n",
        "\n",
        "\n",
        "*   A convex activation function in the first hidden layer\n",
        "*   Convex and non-decreasing activation functions in every subsequent layer\n",
        "* Non-negative weights in every layer beside the first one\n",
        "\n",
        "Convex neural networks were originally proposed by Amos et al. (2017).\n",
        "\n",
        "Sufficient conditions for monotonous neural networks are\n",
        "\n",
        "*  Monotonous activation functions in every layer\n",
        "* Non-negative weights in every layer\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### Tasks\n",
        "\n",
        "* Construct convex FFNNs (C-FFNNs) and monotonous FFNNs (M-FFNNs) by using suitable activation functions and restrictions to the weights of the FFNN.\n",
        "* How do you have to construct M-FFNNs which are monotonous, but not convex?\n",
        "* Calibrate C-FFNNs and M-FFNNs to the datasets introduced above. What do you observe?\n",
        "* For this, consider the following activation functions:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\text{Softplus}:\\qquad \\sigma(x)&=\\log(1+e^x)\n",
        "\\\\\n",
        "\\text{Tanh}:\\qquad \\sigma(x)&=\\operatorname{tanh}(x)=\\frac{e^{2x}-1}{e^{2x}+1}\n",
        "\\\\\n",
        "\\text{Linear}:\\qquad \\sigma(x)&=x\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "* Which of these functions are convex and / or non-decreasing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "QBCtKDNwgjuu",
        "outputId": "87186847-953e-4c93-a323-a1728cd402ba"
      },
      "outputs": [],
      "source": [
        "# Adapt the model name for your plots\n",
        "model_name = 'FFNN'\n",
        "\n",
        "# Number of nodes in each layer\n",
        "units = [32,32,1]\n",
        "\n",
        "# Activation function in each layer\n",
        "# Options: 'softplus', 'tanh', 'relu', 'linear', ...\n",
        "activation = ['softplus','softplus','linear']\n",
        "\n",
        "# non_neg: restrict the weights in different layers to be non-negative\n",
        "non_neg = [False, True, True]\n",
        "\n",
        "# Dataset options: 'bathtub', 'curve', 'double_curve'\n",
        "data = 'bathtub'\n",
        "\n",
        "# epochs: number of iterations in the optimisation process\n",
        "epochs = 1000\n",
        "\n",
        " # Load model\n",
        "model = lm.main_con(units=units, activation=activation, non_neg=non_neg)\n",
        "\n",
        "# Load data\n",
        "xs, ys, xs_c, ys_c = ld.get_data(data)\n",
        "\n",
        "# Calibrate model\n",
        "t1 = now()\n",
        "print(t1)\n",
        "\n",
        "#   set \"verbose=2\" to observe the progress of the calibration process\n",
        "model.optimizer.learning_rate.assign(0.002)\n",
        "h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 0)\n",
        "\n",
        "t2 = now()\n",
        "print('it took', t2 - t1, '(sec) to calibrate the model')\n",
        "\n",
        "# Plot loss and prediction\n",
        "lp.plot_loss(h)\n",
        "lp.plot_data_model(xs, ys, xs_c, ys_c, model, model_name, data, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnvdLfoaZ9SO"
      },
      "source": [
        "## Task 3: Sobolev training\n",
        "\n",
        "We now consider datasets of the form\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{D}=\\big\\{(x_i,\\,y_i,\\,y'_i) \\big\\}_{i=1,...,D}\\,,\\qquad x_i\\in\\mathbb{R},\\,y_i\\in\\mathbb{R},\\,y'_i\\in\\mathbb{R}\\,,\n",
        "\\end{equation}\n",
        "\n",
        "where $y'_i$ is the gradient of the function $y_i$ at the position $x_i$. In this case, a NN can be calibrated both on its original output and on its derivative. For that, the adapted loss function\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\operatorname{MSE}(\\boldsymbol{\\phi},\\,\\mathcal{D})=\\frac{1}{D}\\sum_{d=1}^D&w_{\\text{function}}\\lvert\\lvert \\operatorname{FFNN}(x_i,\\,\\boldsymbol{\\phi})-y_i\\rvert\\rvert_2^2\n",
        "\\\\\n",
        "+&w_{\\text{gradient}}\\Big\\lvert\\Big\\lvert \\frac{\\partial \\operatorname{FFNN}(x_i,\\,\\boldsymbol{\\phi})}{\\partial x_i}-y'_i\\Big\\rvert\\Big\\rvert_2^2\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "is applied. Calibrating a NN on its gradients is usually referred to as *Sobolev training*.\n",
        "<br>\n",
        "\n",
        "### Tasks\n",
        "\n",
        "Apply the following calibration strategies:\n",
        "*  Calibration only on the function itself ($w_{\\text{function}}=1$, $w_{\\text{gradient}}$=0)\n",
        "*  Calibration only on the gradient ($w_{\\text{function}}=0$, $w_{\\text{gradient}}$=1)\n",
        "*  Calibration on the function and its gradient ($w_{\\text{function}}=1$, $w_{\\text{gradient}}$=1)\n",
        "\n",
        "Evaluate the NN model prediction for both the function and its gradient for all three strategies. Furthermore, compare the gradients of standard FFNNs / M-FFNNs / C-FFNNs. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uGQ2-IuOy2Ik",
        "outputId": "08281476-1a22-4bc0-9405-d9ebd577f158"
      },
      "outputs": [],
      "source": [
        "# Adapt the model name for your plots\n",
        "model_name = 'Sobolev'\n",
        "\n",
        "# Number of nodes in each layer\n",
        "units = [32,32,1]\n",
        "\n",
        "# Activation function in each layer\n",
        "# Options: 'softplus', 'tanh', 'relu', 'linear', ...\n",
        "activation = ['tanh','softplus','linear']\n",
        "\n",
        "# non_neg: restrict the weights in different layers to be non-negative\n",
        "non_neg = [True, True, True]\n",
        "\n",
        "# Dataset options: 'bathtub', 'curve', 'double_curve'\n",
        "data = 'double_curve'\n",
        "\n",
        "# epochs: number of iterations in the optimisation process\n",
        "epochs = 3000\n",
        "\n",
        "# Set loss_weights to define the calibration process...\n",
        "# ...[1,0] calibration only on the function\n",
        "# ...[0,1] calibration only on the gradient\n",
        "# ...[1,1] calibration on the function and its gradient\n",
        "loss_weights=[0,1]\n",
        "\n",
        "# Load model\n",
        "model = lm.main_con_grad(loss_weights=loss_weights, \\\n",
        "                         units=units, activation=activation, non_neg=non_neg)\n",
        "\n",
        "# Load data\n",
        "xs, ys, dys, xs_c, ys_c, dys_c = ld.get_data_with_gradients(data)\n",
        "\n",
        "# Calibrate model\n",
        "t1 = now()\n",
        "print(t1)\n",
        "\n",
        "# set \"verbose=2\" to observe the progress of the calibration process\n",
        "model.optimizer.learning_rate.assign(0.002)\n",
        "h = model.fit([xs_c], [ys_c, dys_c], epochs = epochs,  verbose = 0)\n",
        "\n",
        "t2 = now()\n",
        "print('it took', t2 - t1, '(sec) to calibrate the model')\n",
        "\n",
        "# Plot loss and prediction\n",
        "lp.plot_loss(h)\n",
        "lp.plot_data_model_grad(xs, ys, dys, xs_c, ys_c, dys_c, model, model_name, data, 4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
