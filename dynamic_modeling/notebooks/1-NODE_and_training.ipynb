{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAML for dynamic systems:\n",
    "### Neural ODE and training methods for continuous-time models\n",
    "Example system: Nonlinear spring pendulum\n",
    "\n",
    "---\n",
    "\n",
    "Lecture: \"Physics-augmented machine learning\" @ Cyber-Physical Simulation, TU Darmstadt\n",
    "\n",
    "Lecturer: Prof. Oliver Weeger\n",
    "\n",
    "Author: Fabian J. Roth\n",
    "\n",
    "---\n",
    "\n",
    "#### In this notebook, you will...\n",
    "\n",
    "* learn about the two methods to train continuous-time dynamical ML-models: Derivative and trajectory fitting.\n",
    "* learn about neural ODEs and use this model to explore the differences between derivative and trajectory fitting.\n",
    "* use a mixed training strategy to fine tune a neural ODE with trajectory fitting.\n",
    "* calibrate an augmented neural ODE to learn from partial system states.\n",
    "\n",
    "#### Contents\n",
    "1. [Example System: The spring pendulum system](#theory_example_system)<br>\n",
    "    1.1 [Data generation](#data_generation)<br>\n",
    "2. [Theory: Neural ODEs and the model training](#theory_node_training)<br>\n",
    "3. [Task A: Comparing training strategies](#task_a)<br>\n",
    "4. [Task B: Fine tuning with trajectory fitting](#task_b)<br>\n",
    "5. [Homework: Augemented Neural ODE](#homework)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation\n",
    "Run the following code cell to install the required packages. Then **restart your session** to make these packages available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    \"\"\"Determine if the code is running in Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    print(\"Running in Google Colab, trying to install LecturePhysicsAwareML...\")\n",
    "    !git clone --depth 1 https://github.com/Drenderer/LecturePhysicsAwareML.git\n",
    "    %cd LecturePhysicsAwareML/dynamic_modeling\n",
    "    %pip install -e .\n",
    "    print(\n",
    "        \"\\n************************************************************************************\\n\\\n",
    "        Make sure to restart the session after installation (Runtime > Restart Session).\\n\\\n",
    "        ********************************************************************************\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Not running in Google Colab. \\nPlease install the package manually if needed. If you are using pip, run:\\n>>> pip install -e .\\nin the dynamic_modeling directory.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_modeling import ODESolver, NODEDerivative\n",
    "from dynamic_modeling.spring_pendulum import (\n",
    "    SpringPendulum,\n",
    "    animate_spring_pendulum,\n",
    "    polar2cartesian,\n",
    "    plot_trajectory,\n",
    "    plot_energy\n",
    ")\n",
    "\n",
    "import klax\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use jax in float 64 mode (not strictly necessary)\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "# Define global keyword arguments for plotting\n",
    "true_kwargs = dict(color=\"black\", lw=2, marker='o', markevery=20)\n",
    "\n",
    "# Set the seeds for the random number generator\n",
    "key = jr.key(0)\n",
    "mlp_key, loader_key = jr.split(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example System: The spring pendulum system <a id='theory_example_system'></a>\n",
    "In this task we consider a nonlinear spring pendulum system:\n",
    "\n",
    "<img src=\"images\\Spring_pendulum_only_cartesian.png\" height=\"400\"/>\n",
    "\n",
    "The governing equations can be obtained in many ways, here we use [Hamiltons Principle](https://en.wikipedia.org/wiki/Hamilton%27s_principle) and the [Euler-Lagrange equation](https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation):\n",
    "\n",
    "Describing position of the mass $m$ with generalized (cartesian) coordinates $q_x, q_y$ and their velocities $\\dot q_x, \\dot q_y$, the total kinetic energy $T$ and potential energy $U$ are given by\n",
    "$$\n",
    "\\begin{align}\n",
    "    T = \\frac{1}{2}m \\dot q_x^2 + \\frac{1}{2}m \\dot q_y^2, && U = \\frac{1}{2}k\\big(\\sqrt{q_x^2+q_y^2} - l_0\\big)^2 + mgq_y.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $k$, $l=\\sqrt{q_x^2+q_y^2}$ and $l_0$ denote the spring constant, length and rest length respectively, and $g$ is the gravitational constant.\n",
    "\n",
    "We thus obtain the Lagrangian $\\mathcal{L}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(q_x, q_y, \\dot q_x, \\dot q_y) = T - U = \\frac{1}{2}m(\\dot q_x^2 + \\dot q_y^2) - \\left[ \\frac{1}{2}k\\left(\\sqrt{q_x^2 + q_y^2} - l_0\\right)^2 + mgq_y \\right] + C\n",
    "\\end{align}\n",
    "$$\n",
    "where $C$ is an arbitrary constant corresponding to the chosen zero-level of the potential energy.\n",
    "\n",
    "Applying the [Euler-Lagrange equation](https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation) results in the following equation of motion:\n",
    "$$\n",
    "\\begin{align}\n",
    "0 &= m\\ddot q_x + k\\left(1 - \\frac{l_0}{\\sqrt{q_x^2 + q_y^2}}\\right)q_x\\\\\n",
    "0 &= m\\ddot q_y + k\\left(1 - \\frac{l_0}{\\sqrt{q_x^2 + q_y^2}}\\right)q_y + mg\n",
    "\\end{align}\n",
    "$$\n",
    "This can be written as a first order ordinary differential equation (ODE) by introducing the velocities $v_x = \\dot q_x, v_y = \\dot q_y$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dot q_x &= v_x\\\\\n",
    "\\dot q_y &= v_y\\\\\n",
    "\\dot v_x &= -\\frac{k}{m}\\left(1 - \\frac{l_0}{l(\\boldsymbol{q})}\\right)q_x\\\\\n",
    "\\dot v_y &= -\\frac{k}{m}\\left(1 - \\frac{l_0}{l(\\boldsymbol{q})}\\right)q_y - g\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's generate some trajectories, using varying initial positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data generation <a id='data_generation'></a>\n",
    "The following code cells generate and visualize data from the spring pendulum system.\n",
    "\n",
    "Feel free to change the parameters marked by ``# <<<`` *later* during the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_pendulum = SpringPendulum(k=2.0, m=1.0, g=1.0, l0=1.0)\n",
    "true_system = ODESolver(spring_pendulum)\n",
    "\n",
    "num_traj = 40           # <<< Number of trajectories\n",
    "t_max = 5               # <<< Length of trajectories [s]\n",
    "num_t_samples = 100     # <<< Number of samples per trajectory\n",
    "\n",
    "key = jr.key(0)\n",
    "r_key, θ_key = jr.split(key)\n",
    "ts = jnp.linspace(0, t_max, num_t_samples)\n",
    "\n",
    "# Generate random initial conditions for training\n",
    "# Here we use polar coordinates to more easily define reasonable initial conditions.\n",
    "r0s = jr.uniform(r_key, (num_traj,), minval=0.9, maxval=1.5)\n",
    "θ0s = jr.uniform(θ_key, (num_traj,), minval=-jnp.pi / 6, maxval=jnp.pi / 6)\n",
    "y0s_polar = jnp.stack([r0s, θ0s, jnp.zeros((num_traj,)), jnp.zeros((num_traj,))], axis=-1)\n",
    "y0s = polar2cartesian(y0s_polar)\n",
    "ys = jax.vmap(true_system, in_axes=(None, 0))(ts, y0s)\n",
    "\n",
    "# Make derivative data\n",
    "ys_flat = jnp.reshape(ys, (-1, ys.shape[-1]))\n",
    "ys_t_flat = jax.vmap(spring_pendulum, (None, 0))(None, ys_flat)\n",
    "\n",
    "# Plot a sample trajectory\n",
    "plot_trajectory(ts, ys[0], states=\"all\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate all trainig trajectories\n",
    "animate_spring_pendulum(ts, ys, speedup=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory: Neural ODEs and the model training <a id='theory_node_training'></a>\n",
    "\n",
    "#### Neural ODEs (NODEs)\n",
    "In this tutorial we consider models of the form \n",
    "$$ \\dot{\\mathbf{y}} = f_{NN}(\\mathbf{y}), \\qquad \\text{with} \\;\\mathbf{y}=[q_x, q_y, v_x, v_y]^\\intercal$$\n",
    "where the function $f_{NN}: \\mathbb{R}^n\\mapsto\\mathbb{R}^n$ is parameterized by a neural network with parameters $\\mathbf{\\phi}$. We want to learn these parameters from data. In particular, we consider neural ODEs which simply use a neural network directly as function $f_{NN}(\\mathbf{y}) = NN(\\mathbf{y}; \\mathbf{\\phi})$.\n",
    "\n",
    "#### Training: Derivative vs. Trajectory fitting\n",
    "\n",
    "When it comes to training such models we have two different options: *Derivative* and *Trajectory* fitting:\n",
    "1. **Derivative fitting**\n",
    "\n",
    "  Here we need the data to be given as pairs $(\\mathbf{y}, \\mathbf{\\dot y})$, with $\\mathbf{y}\\in\\mathbb{R}^n$. So for every state of the system $\\mathbf{y}$ we have the corresponding change of the state $\\mathbf{\\dot y}$. We have such data for multiple states forming   our data set: $\\{(\\mathbf{y}_d, \\mathbf{\\dot y}_d)\\}_{d=0,...,D}$. With this data we can simply train the function $f_{NN}(\\mathbf{y}; \\mathbf{\\phi})$ directly. This is called derivative fitting and  we can use a loss function like this:\n",
    "  $$L(\\mathbf{\\phi}) = \\frac{1}{D}\\sum_{d=1}^D||f_{NN}(\\mathbf{y_d}; \\mathbf{\\phi}) - \\mathbf{\\dot y_d}||^2$$\n",
    "\n",
    "2. **Trajectory fitting**\n",
    "\n",
    "  Most of the time we have data in the form of trajectories, meaning that we have a list of states with corresponding timestamps: $\\{(t_k, \\mathbf{y}_k)\\}_{k=0,...,K}$. Here $k$ is the index of the timestamp. Most of the time these timestamps are equidistant, but this is not required. Such data is commonly obtained via measurements or simulations. Usually we also have multiple trajectories, which we index by $d$: $\\{\\{(t_k^d, \\mathbf{y}_k^d)\\}_{k=0,...,K}\\}_{d=0,...,D}$.\n",
    "\n",
    "  Because we don't have access to the derivatives $\\mathbf{\\dot y}$ directly, we first need to integrate our model to obtain a trajectory prediction: \n",
    "  $$\\tilde{\\mathbf{y}}_k^d(\\mathbf{\\phi}) = \\int_{t_0}^{t_k}f_{NN}(\\mathbf{y(t)}; \\mathbf{\\phi})dt, \\quad \\mathbf{y(0)} = \\mathbf{y_0^d}$$\n",
    "  We can now compare this predicted trajectory with the given data and formulate a loss like this:\n",
    "  $$L(\\mathbf{\\phi}) = \\frac{1}{D}\\sum_{d=1}^D \\frac{1}{K}\\sum_{k=1}^K||\\tilde{\\mathbf{y}}_k^d(\\mathbf{\\phi}) - \\mathbf{y}_k^d||^2$$\n",
    "\n",
    "In the following code cell we define the loss functions for derivative and trajectory fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions for derivative and trajectory fitting\n",
    "\n",
    "def derivative_loss(model: ODESolver, data, batch_axis):\n",
    "    \"\"\"Evaluate the models derivative function and compare it to the true derivative.\"\"\"\n",
    "    ys, ys_t = data\n",
    "    ys_t_pred = jax.vmap(model.func, in_axes=batch_axis)(None, ys)\n",
    "    return jnp.mean(jnp.square(ys_t_pred - ys_t))\n",
    "\n",
    "\n",
    "def trajectory_loss(model: ODESolver, data, batch_axis):\n",
    "    \"\"\"Compute the models predicted trajectory and compare it to the true trajectory.\"\"\"\n",
    "    ts, ys = data\n",
    "    ys_pred = jax.vmap(model, in_axes=batch_axis)(ts, ys[:, 0])\n",
    "    return jnp.mean(jnp.square(ys_pred - ys))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Task A: Comparing training strategies <a id='task_a'></a>\n",
    "\n",
    "Before running the code: Feel free to change some parameters of the data generation process above (e.g. ``t_max``) or the parameters of the neural ODE below. Parameters you can change are marked with comments like this: ``# <<<``.\n",
    "\n",
    "With a partner (one person does (a) the other (b)) run the code in the cells below:\n",
    "\n",
    "a) Derivative fitting (DF)\n",
    "\n",
    "b) Trajectory fitting (TF)\n",
    "\n",
    "and then plot the NODE prediction(s) (3rd code cell below). Consider the following questions:\n",
    "\n",
    "1) What differences between the training strategies do you observe?\n",
    "\n",
    "2) Would it make sense to choose the activation ``relu`` for the NODEs NN?\n",
    "\n",
    "3) Pay particular attention to *extrapolation* in time by evaluating the models on time series that extend beyond the duration of the training data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative fitting (DF)\n",
    "node_deriv = NODEDerivative(\n",
    "    hidden_layer_sizes=[16, 16],    # <<< NN size\n",
    "    activation=\"softplus\",          # <<< Activation function \"softplus\", \"relu\" or \"sigmoid\"\n",
    "    key=mlp_key\n",
    ")\n",
    "node_df = ODESolver(node_deriv)\n",
    "node_df, hist = klax.fit(\n",
    "    node_df,\n",
    "    (ys_flat, ys_t_flat),\n",
    "    batch_size=32,\n",
    "    steps=25_000,\n",
    "    loss_fn=derivative_loss,\n",
    "    key=loader_key,\n",
    ")\n",
    "hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory fitting (TF)\n",
    "node_deriv = NODEDerivative(\n",
    "    hidden_layer_sizes=[16, 16],    # <<< NN size\n",
    "    activation=\"softplus\",          # <<< Activation function \"softplus\", \"relu\" or \"sigmoid\"\n",
    "    key=mlp_key\n",
    ")\n",
    "node_tf = ODESolver(node_deriv)\n",
    "\n",
    "node_tf, hist = klax.fit(\n",
    "    node_tf,\n",
    "    (ts, ys),\n",
    "    batch_axis=(None, 0),\n",
    "    batch_size=16,\n",
    "    steps=10_000,\n",
    "    loss_fn=trajectory_loss,\n",
    "    key=loader_key,\n",
    ")\n",
    "hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the NODE prediction(s)\n",
    "y0_eval = jnp.array([0.7, -1.2, 0.0, 0.0])  # <<< Initial condition for evaluation, with [q_x, q_y, v_x, v_y]\n",
    "t_max_eval = 20                             # <<< Length of evaluation trajectory [s], choose larger values to test extrapolation in time\n",
    "states = \"positions\"                        # <<< Plot only: \"positions\", \"velocities\" or \"all\"\n",
    "ts_eval = jnp.linspace(0, t_max_eval, 1000)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "ys_true = true_system(ts_eval, y0_eval)\n",
    "plot_trajectory(ts_eval, ys_true, ax=axes[0], label=\"True State\", states=states, **true_kwargs)\n",
    "E_true = jax.vmap(spring_pendulum.get_energy)(ys_true)\n",
    "plot_energy(ts_eval, E_true, ax=axes[1], label=\"True Energy\", **true_kwargs)   # type: ignore\n",
    "try:\n",
    "    ys_pred_df = node_df(ts_eval, y0_eval)\n",
    "    plot_trajectory(ts_eval, ys_pred_df, ax=axes[0], states=states, color=\"red\", label=\"Predicted (DF)\")\n",
    "    E_pred_df = jax.vmap(spring_pendulum.get_energy)(ys_pred_df)\n",
    "    plot_energy(ts_eval, E_pred_df, ax=axes[1], color=\"red\", label=\"Energy of prediction (DF)\")   # type: ignore\n",
    "except NameError:\n",
    "    print(\"You have not trained ys_pred_df (derivative fitting). Skipping this model.\")\n",
    "try:\n",
    "    ys_pred_tf = node_tf(ts_eval, y0_eval)\n",
    "    plot_trajectory(ts_eval, ys_pred_tf, ax=axes[0], states=states, color=\"green\", label=\"Predicted (TF)\")\n",
    "    E_pred_tf = jax.vmap(spring_pendulum.get_energy)(ys_pred_tf)\n",
    "    plot_energy(ts_eval, E_pred_tf, ax=axes[1], color=\"green\", label=\"Energy of prediction (TF)\")   # type: ignore\n",
    "except NameError:\n",
    "    print(\"You have not trained ys_pred_tf (trajectory fitting). Skipping this model.\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate the true solution (blue) and the models solution (gray)\n",
    "_prediction = ys_pred_df    # <<< Put the model you've trained here to view it animated (ys_pred_df or ys_pred_tf)\n",
    "_temp = jnp.stack([ys_true, _prediction], axis=0)\n",
    "animate_spring_pendulum(ts_eval, _temp, speedup=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Task B: Fine tuning with trajectory fitting <a id='task_b'></a>\n",
    "\n",
    "The predictions from the previous task might not be satisfactory. Here we try to improve upon the results by using longer trajectories and trajectory fitting to fine tune a model.\n",
    "\n",
    "Run the following code to train a neural ODE in two steps:\n",
    "- Using derivative fitting with the original data\n",
    "- Continue training the neural ODE by trajectory fitting to longer trajectories.\n",
    "\n",
    "While the models train, consider the following (alone or with a partner):\n",
    "\n",
    "1) How to you expect the results to differ from the previous task, after fine-tuning?\n",
    "\n",
    "2) Explain why you hold these expectations.\n",
    "\n",
    "After the model trained:\n",
    "\n",
    "3) Look at the resulting predictions. Do they improve upon the results from the previous task. If so, in which way?\n",
    "\n",
    "4) In theory, could the same results be obtained by using only trajectory fitting with the longer trajectories? What is the purpose of the derivative fitting here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate longer trajectories for fine tuning.\n",
    "\n",
    "num_traj = 40           # <<< Number of trajectories\n",
    "t_max = 20              # <<< Length of trajectories [s]\n",
    "num_t_samples = 200     # <<< Number of samples per trajectory\n",
    "\n",
    "key = jr.key(0)\n",
    "r_key, θ_key = jr.split(key)\n",
    "ts_tune = jnp.linspace(0, t_max, num_t_samples)\n",
    "\n",
    "# Generate random initial conditions for training\n",
    "r0s = jr.uniform(r_key, (num_traj,), minval=0.9, maxval=1.5)\n",
    "θ0s = jr.uniform(θ_key, (num_traj,), minval=-jnp.pi / 6, maxval=jnp.pi / 6)\n",
    "y0s_polar = jnp.stack([r0s, θ0s, jnp.zeros((num_traj,)), jnp.zeros((num_traj,))], axis=-1)\n",
    "y0s_tune = polar2cartesian(y0s_polar)\n",
    "ys_tune = jax.vmap(true_system, in_axes=(None, 0))(ts_tune, y0s_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Animate the longer trajectories\n",
    "animate_spring_pendulum(ts_tune, ys_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_deriv = NODEDerivative(\n",
    "    hidden_layer_sizes=[16, 16],\n",
    "    activation=\"softplus\",\n",
    "    key=mlp_key\n",
    ")\n",
    "node_tuned = ODESolver(node_deriv)\n",
    "\n",
    "# Start with derivative fitting (DF)\n",
    "node_tuned, hist = klax.fit(\n",
    "    node_tuned,\n",
    "    (ys_flat, ys_t_flat),   # Use the old data\n",
    "    batch_size=32,\n",
    "    steps=25_000,\n",
    "    loss_fn=derivative_loss,\n",
    "    key=loader_key,\n",
    ")\n",
    "\n",
    "# Continue with trajectory fitting (TF)\n",
    "node_tuned, hist = klax.fit(\n",
    "    node_tuned,\n",
    "    (ts_tune, ys_tune),     # Use the tuneing data for TF\n",
    "    batch_axis=(None, 0),\n",
    "    batch_size=16,\n",
    "    steps=2_000,\n",
    "    loss_fn=trajectory_loss,\n",
    "    key=loader_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the NODE prediction(s)\n",
    "\n",
    "y0_eval = jnp.array([0.4, -1.2, 0.0, 0.0])  # <<< Initial condition for evaluation, with [q_x, q_y, v_x, v_y]\n",
    "t_max_eval = 50                             # <<< Length of evaluation trajectory\n",
    "states = \"positions\"                        # <<< Plot only: \"positions\", \"velocities\" or \"all\"\n",
    "\n",
    "ts_eval = jnp.linspace(0, t_max_eval, 1000)\n",
    "\n",
    "# Compute the true and predicted trajectories\n",
    "ys_true = true_system(ts_eval, y0_eval)\n",
    "ys_pred_tuned = node_tuned(ts_eval, y0_eval)\n",
    "\n",
    "# Compute the energies of the states\n",
    "E_true = jax.vmap(spring_pendulum.get_energy)(ys_true)\n",
    "E_predicted_df = jax.vmap(spring_pendulum.get_energy)(ys_pred_tuned)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "plot_trajectory(ts_eval, ys_true, ax=axes[0], states=states, label=\"True State\", **true_kwargs)    # type: ignore\n",
    "plot_energy(ts_eval, E_true, ax=axes[1], label=\"True Energy\", **true_kwargs)   # type: ignore\n",
    "\n",
    "plot_trajectory(ts_eval, ys_pred_tuned, ax=axes[0], states=states, color=\"blue\", label=\"Predicted (DF+TF)\")\n",
    "plot_energy(ts_eval, E_predicted_df, ax=axes[1], color=\"blue\", label=\"Energy of prediction (DF+TF)\")   # type: ignore\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate the true solution (blue) and the models solution (gray)\n",
    "temp = jnp.stack([ys_true, ys_pred_tuned], axis=0)\n",
    "animate_spring_pendulum(ts_eval, temp, speedup=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Homework: Augemented Neural ODE <a id='homework'></a>\n",
    "So far we have used training data of the full state $\\mathbf{y}=[q_x, q_y, v_x, v_y]^\\intercal$. However, in practice we often don't have the velocities $v_i$ available. Furthermore, when we use ML approaches to learn the dynamics of complex systems, then we often only have a subset of the full system state available. To simulate such a situation the following task assumes that only the positions (but not the velocities) are available in the training data. \n",
    "\n",
    "1) Run the following code. It generates truncated data and trains a neural ODE with a two-dimensional state vector $\\mathbf{y}=[q_x, q_y]^\\intercal$. Can the neural ODE learn the dynamics of the pendulum using only a two dimensional state vector? Explain the results.\n",
    "\n",
    "It is possible to \"artificially\" increase the dimensionality of the neural ODE's state vector, to make it larger than the data's state size. For this we add so calld augmented states/dimensions to the state vector:\n",
    "$$\\mathbf{y} = \\begin{bmatrix} \\mathbf{y}_{data}\\\\\\mathbf{y}_{augmented} \\end{bmatrix}$$\n",
    "We then only use $\\mathbf{y}_{data}$ to calculate the loss. The augmented states provide more flexibility to our model that it can use to learn more complicated dynamics.\n",
    "\n",
    "2) Is it possible to use derivative fitting when using augmented states? Explain.\n",
    "\n",
    "3) How many augmented states are necessary (in theory) for the model to be able to learn the spring pendulum dynamics? Change the line marked with ``# <<< Number of augmented states...`` and train the model again. What do you observe?\n",
    "\n",
    "Note: To achieve good results you might need to train the models for a long time (~30-60 min). So go grab a coffee or something while waiting for the training results :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_pendulum = SpringPendulum(k=2.0, m=1.0, g=1.0, l0=1.0)\n",
    "true_system = ODESolver(spring_pendulum)\n",
    "\n",
    "n = 40      # Number of trajectories\n",
    "t_max = 10  # Length of trajectories [s]\n",
    "num_t_samples = 200     # <<< Number of samples per trajectory\n",
    "\n",
    "key = jr.key(0)\n",
    "r_key, θ_key = jr.split(key)\n",
    "ts = jnp.linspace(0, t_max, num_t_samples)\n",
    "\n",
    "# Generate random initial conditions for training\n",
    "# Here we use polar coordinates to more easily define reasonable initial conditions.\n",
    "r0s = jr.uniform(r_key, (n,), minval=0.9, maxval=1.5)\n",
    "θ0s = jr.uniform(θ_key, (n,), minval=-jnp.pi / 6, maxval=jnp.pi / 6)\n",
    "y0s = jnp.stack([r0s, θ0s, jnp.zeros((n,)), jnp.zeros((n,))], axis=-1)\n",
    "y0s = polar2cartesian(y0s)\n",
    "ys = jax.vmap(true_system, in_axes=(None, 0))(ts, y0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the velocities from the training data\n",
    "ys_truncated = ys[:, :, :2] # Use only the first two states.\n",
    "print(f\"Training data shape (number of trajectories, number of timestamps per trajectory, number of states)={ys_truncated.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_states = ys_truncated.shape[-1]\n",
    "num_augmented_states = 0    # <<< Number of augmented states that are added to the data states\n",
    "num_total_states = num_data_states + num_augmented_states\n",
    "print(f\"Total number of states: {num_total_states}.\")\n",
    "\n",
    "node_deriv = NODEDerivative(\n",
    "    state_size=num_total_states,\n",
    "    key=mlp_key\n",
    ")\n",
    "augmented_node = ODESolver(\n",
    "    node_deriv, \n",
    "    augmentation=num_augmented_states    # Add to augmented states to the model\n",
    ") \n",
    "\n",
    "def trajectory_loss(model: ODESolver, data, batch_axis):\n",
    "    \"\"\"Compute the models predicted trajectory and compare it to the true trajectory.\"\"\"\n",
    "    ts, ys = data\n",
    "    ys_pred = jax.vmap(model, in_axes=batch_axis)(ts, ys[:, 0])\n",
    "    return jnp.mean(jnp.square(ys_pred - ys))\n",
    "\n",
    "augmented_node, hist = klax.fit(\n",
    "    augmented_node,\n",
    "    (ts, ys_truncated),\n",
    "    batch_axis=(None, 0),\n",
    "    steps=50_000,\n",
    "    loss_fn=trajectory_loss,\n",
    "    optimizer=optax.adam(4e-4),\n",
    "    key=loader_key,\n",
    ")\n",
    "hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the NODE prediction(s)\n",
    "\n",
    "y0_eval = jnp.array([0.2, -1.2, 0.0, 0.0])  # <<< Initial condition for evaluation\n",
    "t_max_eval = 10                             # <<< Length of evaluation trajectory\n",
    "ts_eval = jnp.linspace(0, t_max_eval, 1000)\n",
    "\n",
    "# Compute the true and predicted trajectories\n",
    "ys_true = true_system(ts_eval, y0_eval)\n",
    "ys_pred_augmented = augmented_node(ts_eval, y0_eval[:2])\n",
    "\n",
    "# Compute the energies of the states\n",
    "E_true = jax.vmap(spring_pendulum.get_energy)(ys_true)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "plot_trajectory(ts_eval, ys_true, label=\"True State\", **true_kwargs)    # type: ignore\n",
    "plot_trajectory(ts_eval, ys_pred_augmented, color=\"blue\", label=\"Predicted (augmented NODE)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
