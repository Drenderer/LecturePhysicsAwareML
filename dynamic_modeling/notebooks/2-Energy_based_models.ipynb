{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAML for dynamic systems:\n",
    "### Energy-based continuous-time models\n",
    "Example system: Two mass oscillator\n",
    "\n",
    "---\n",
    "\n",
    "Lecture: \"Physics-augmented machine learning\" @ Cyber-Physical Simulation, TU Darmstadt\n",
    "\n",
    "Lecturer: Prof. Oliver Weeger\n",
    "\n",
    "Author: Fabian J. Roth\n",
    "\n",
    "---\n",
    "\n",
    "#### In this notebook, you will...\n",
    "\n",
    "* Calibrate the energy-based continuous-time models: Hamiltonian, Lagrangian and port-Hamiltonian neural networks.\n",
    "* Learn about canonical coordinates and their impact on the training of Hamiltonian coordinates.\n",
    "* Train the models with data from a dissipative system\n",
    "* Investigate the use of convex neural networks to bias a port-Hamiltonian neural network towards stability.\n",
    "\n",
    "#### Contents\n",
    "1. [Example System: The two mass oscillator](#theory_example_system)<br>\n",
    "    1.1 [Data generation](#data_generation)<br>\n",
    "2. [Theory: Hamiltonian and Lagrangian neural networks](#theory_hnn_lnn)<br>\n",
    "3. [Task A: Comparing HNNs and LNNs](#task_1)<br>\n",
    "    3.1 [Task A.1: Training](#task_1_1)<br>\n",
    "    3.2 [Task A.2: Different system parameters](#task_1_2)<br>\n",
    "4. [Theory: Energy dissipation](#theory_phnn)<br>\n",
    "5. [Task B: (s)PHNNs](#task_2)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation\n",
    "Run the following code cell to install the required packages. Then **restart your session** to make these packages available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    \"\"\"Determine if the code is running in Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    print(\"Running in Google Colab, trying to install LecturePhysicsAwareML...\")\n",
    "    !git clone --depth 1 https://github.com/Drenderer/LecturePhysicsAwareML.git\n",
    "    %cd LecturePhysicsAwareML/dynamic_modeling\n",
    "    %pip install -e .\n",
    "    print(\n",
    "        \"\\n************************************************************************************\\n\\\n",
    "        Make sure to restart the session after installation (Runtime > Restart Session).\\n\\\n",
    "        ********************************************************************************\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Not running in Google Colab. \\nPlease install the package manually if needed. If you are using pip, run:\\n>>> pip install -e .\\nin the dynamic_modeling directory.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_modeling import (\n",
    "    ODESolver\n",
    ")\n",
    "from dynamic_modeling.two_mass_oscillator import (\n",
    "    TwoMassOscillator,\n",
    "    plot_trajectory,\n",
    "    plot_energy,\n",
    ")\n",
    "from dynamic_modeling.models import ACTIVATIONS\n",
    "import klax\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "from jaxtyping import PRNGKeyArray, Array\n",
    "from typing import Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use jax in float 64 mode (not strictly necessary)\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "# Define global keyword arguments for plotting\n",
    "true_kwargs = dict(color=\"black\", lw=2, marker='o', markevery=20)\n",
    "\n",
    "# Set the seeds for the random number generator\n",
    "key = jr.key(0)\n",
    "model_key, loader_key = jr.split(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example System: The two mass oscillator <a id='theory_example_system'></a>\n",
    "In this task we consider a simple linear two mass oscillator system:\n",
    "\n",
    "<img src=\"images\\Two_mass_oscillator.png\" height=\"200\"/>\n",
    "\n",
    "Describing position of the masses $m_1$ and $m_2$ with generalized coordinates $q_1, q_2$ and their velocities $\\dot q_1, \\dot q_2$, the total kinetic energy $T$ and potential energy $U$ are given by\n",
    "$$\n",
    "\\begin{align}\n",
    "    T = \\frac{1}{2}m_1 \\dot q_1^2 + \\frac{1}{2}m_2 \\dot q_2^2, && U = \\frac{1}{2}k_1q_1^2 + \\frac{1}{2}k_2(q_2 - q_1)^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with spring constants $k_1$ and $k_2$.\n",
    "\n",
    "In the last exercise we used the Euler-Lagrange equation to obtain the equations of motion. So naturally, this time we'll use the Hamiltonian framework. This requires us to use the generalized momenta $p_i = m_i \\dot q_i$ instead of the velocities $v_i=\\dot q_i$. \n",
    "By substituting $q_i = \\frac{p_i}{m_i}$ we can formulate the Hamiltonian $\\mathcal{H}$ as the total energy:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathcal{H}(q_1, q_2, p_1, p_2) = T + U = \\frac{1}{2m_1}p_1^2 + \\frac{1}{2m_2}p_2^2 + \\frac{1}{2}k_1q_1^2 + \\frac{1}{2}k_2(q_2 - q_1)^2 + C\n",
    "\\end{align}\n",
    "$$\n",
    "where $C$ is an arbitrary constant corresponding to the chosen zero-level of the potential energy.\n",
    "\n",
    "Now we use Hamiltons equations of motion\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial q_i}{\\partial t} &= \\phantom{-}\\frac{\\partial \\mathcal{H}}{\\partial p_i}, \\qquad\\qquad i=1,2\\\\\n",
    "    \\frac{\\partial p_i}{\\partial t} &= -\\frac{\\partial \\mathcal{H}}{\\partial q_i}, \\qquad\\qquad i=1,2\n",
    "\\end{align}\n",
    "$$\n",
    "and plug in our $\\mathcal{H}$. After rewriting it in terms of velocities $v_1 = \\frac{p_1}{m_1}, v_y = \\frac{p_2}{m_2}$ we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dot q_1 &= v_x\\\\\n",
    "\\dot q_2 &= v_y\\\\\n",
    "\\dot v_1 &= -\\frac{k_1 + k_2}{m_1}q_1 + \\frac{k_2}{m_1}q_1\\\\\n",
    "\\dot v_2 &= \\frac{k_2}{m_2}q_1 - \\frac{k_2}{m_2}q_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's generate some trajectories, using varying initial positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data generation <a id='data_generation'></a>\n",
    "The following code cells generate and visualize data from the spring pendulum system.\n",
    "\n",
    "Feel free to change the parameters marked by ``# <<<`` *later* during the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_mass_oscillator = TwoMassOscillator(m1=1.0, m2=1.0, k1=1.0, k2=0.5, c1=0.0, c2=0.0) # Change this in later tasks\n",
    "true_system = ODESolver(two_mass_oscillator)\n",
    "\n",
    "num_traj = 20           # <<< Number of trajectories\n",
    "t_max = 10              # <<< Length of trajectories [s]\n",
    "num_t_samples = 500     # <<< Number of samples per trajectory\n",
    "\n",
    "key = jr.key(0)\n",
    "r_key, Î¸_key = jr.split(key)\n",
    "ts = jnp.linspace(0, t_max, num_t_samples)\n",
    "\n",
    "# Generate random initial conditions for training\n",
    "q0s = jr.uniform(r_key, (num_traj, 2), minval=-1.0, maxval=1.0)\n",
    "q_t0s = jnp.zeros_like(q0s)\n",
    "y0s = jnp.concat([q0s, q_t0s], axis=-1)\n",
    "ys = jax.vmap(true_system, in_axes=(None, 0))(ts, y0s)\n",
    "\n",
    "# Make derivative data\n",
    "ys_flat = jnp.reshape(ys, (-1, ys.shape[-1]))\n",
    "ys_t_flat = jax.vmap(two_mass_oscillator, (None, 0))(None, ys_flat)\n",
    "\n",
    "# Plot an example trajectory\n",
    "trajectory_index = 1    # <<< Plot trajectory with index (0,...,num_traj-1)\n",
    "plot_trajectory(ts, ys[trajectory_index], states=\"all\") # type:ignore\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory: Hamiltonian and Lagrangian neural networks <a id='theory_hnn_lnn'></a>\n",
    "In this task we will train and compare Hamiltonian and Lagrangian neural netwoks.\n",
    "### 2.1 Hamiltonian neural networks\n",
    "Similar to a nerual ODE Hamiltonian neural networks parameterize a first order ODE $\\dot{\\mathbf{y}} = f_{NN}(\\mathbf{y})$. However, $f_{NN}$ is not parameterized with a neural network directly. Instead, we use Hamiltons equations of motion:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\dot{\\mathbf{y}} = f_{NN}(\\mathbf{y}) = \\underbrace{\\begin{bmatrix}\\mathbf{0}&\\mathbf{I}\\\\-\\mathbf{I}&\\mathbf{0}\\end{bmatrix}}_{\\mathbf{\\Omega}}\\frac{\\partial\\mathcal{H}(\\mathbf{y})}{\\partial\\mathbf{y}}\n",
    "\\end{equation}\n",
    "$$\n",
    "with the *Hamiltonian* function $\\mathcal{H}(\\mathbf{y}):\\;\\mathbb{R}^n\\mapsto\\mathbb{R}$, which represents the total energy of the system $\\mathcal{H} = T+ U$.\n",
    "The Hamiltonian modeling framework requires the use of so called *canonical coordinates* this means that the state vector $\\mathbf{y}$ should consist of the generalized positions $\\mathbf{q}$ and the corresponding generalized momenta $\\mathbf{p}$:\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}\\mathbf{q}\\\\\\mathbf{p}\\end{bmatrix}.\n",
    "$$\n",
    "We can thus rewrite the equation of motion as:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\dot{\\mathbf{q}} = \\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{p}} \\qquad\n",
    "    \\dot{\\mathbf{p}} = -\\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{q}}\n",
    "\\end{align}\n",
    "$$\n",
    "By parameterizing the Hamiltonian using a neural network $\\mathcal{H}_{NN} = NN(\\mathbf{y}; \\mathbf{\\phi})$ with parameters $\\mathbf{\\phi}$ we obtain a *Hamiltonian neural network*.\n",
    "\n",
    "Hamiltonian neural networks are constrained to conserve the Hamiltonian, which follows directly from using Hamiltons equations of motion:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\dot{\\mathcal{H}}(\\mathbf{y}(t)) = \\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}} \\cdot \\dot{\\mathbf{y}} = \\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}}^\\intercal\\mathbf{\\Omega}\\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}} = 0\n",
    "\\end{align}\n",
    "$$\n",
    "The last equatlity is a consequence of the fact that the canonical symplectic matrix $\\mathbf{\\Omega}$ is skew-symmetric ($\\mathbf{\\Omega}=-\\mathbf{\\Omega}^\\intercal$), which implies $\\mathbf{x}^\\intercal\\mathbf{\\Omega}\\mathbf{x}=0,\\;\\forall\\mathbf{x}\\in\\mathbb{R}^n$\n",
    "\n",
    "### 2.2 Lagrangian neural networks\n",
    "Instead of the Hamiltonian, we can also use the Lagrangian modeling framework to formulate a constraint continuous-time dynamical ML model. Here we use the Euler-Lagrange equation as a starting point:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{q}} - \\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\partial\\mathcal{L}}{\\partial\\dot{\\mathbf{q}}} = 0.\n",
    "\\end{equation}\n",
    "$$\n",
    "Here $\\mathcal{L}(\\mathbf{q}, \\dot{\\mathbf{q}}):\\;\\mathbb{R}^n\\mapsto\\mathbb{R}$ is the *Lagrangian*. It is given by the difference of the kinetic and potential energies $\\mathcal{L} = T - U$. By propagating the time derivative and rearranging we get:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\ddot{q} = (\\nabla_{\\dot{q}}\\nabla_{\\dot{q}}^T\\mathcal{L})^{-1}[\\nabla_q\\mathcal{L}-(\\nabla_q\\nabla_{\\dot{q}}^T\\mathcal{L})\\dot{q}].\n",
    "\\end{equation}\n",
    "$$\n",
    "Here we use the Nabla notation: $\\nabla_x =\\frac{\\partial}{\\partial x}$. Similar to before, we parameterize the Lagrangian using a neural network $\\mathcal{L}_{NN} = NN(\\mathbf{y}; \\mathbf{\\phi})$ with parameters $\\mathbf{\\phi}$ and obtain a *Lagrangian neural network*.\n",
    "It can be shown that the Euler-Lagrange equation also implies conservation of the total energy $E=T+U$ if $\\mathcal{L} = T - U$. In fact, the Lagrangian and Hamiltonian frameworks are mathematically equivalent (The Hamiltonian can be calculated from the Lagrangian and vice versa).\n",
    "\n",
    "\n",
    "The following code cell implements the derivative functions of Hamiltonian and Lagrangian neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HamiltonianNN(eqx.Module):\n",
    "    hamiltonian: klax.nn.MLP\n",
    "\n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 state_size: int = 4,\n",
    "                 hidden_layer_sizes: list[int] = [16, 16],\n",
    "                 activation: Literal[\"softplus\", \"relu\", \"sigmoid\"] = \"softplus\",\n",
    "                 key: PRNGKeyArray):\n",
    "        self.hamiltonian = klax.nn.MLP(\n",
    "            in_size=state_size,\n",
    "            out_size=\"scalar\",\n",
    "            width_sizes=hidden_layer_sizes,\n",
    "            activation=ACTIVATIONS[activation],\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t: Array, y: Array, u: Array | None = None) -> Array:\n",
    "        grad_H = jax.grad(self.hamiltonian)(y)\n",
    "        dH_dq, dH_dp = jnp.split(grad_H, 2, axis=-1)\n",
    "        dq_dt = dH_dp\n",
    "        dp_dt = -dH_dq\n",
    "        return jnp.concatenate([dq_dt, dp_dt], axis=-1)\n",
    "    \n",
    "class LagrangianNN(eqx.Module):\n",
    "    lagrangian: klax.nn.MLP\n",
    "\n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 state_size: int = 4,\n",
    "                 hidden_layer_sizes: list[int] = [16, 16],\n",
    "                 activation: Literal[\"softplus\", \"relu\", \"sigmoid\"] = \"softplus\",\n",
    "                 key: PRNGKeyArray):\n",
    "        self.lagrangian = klax.nn.MLP(\n",
    "            in_size=state_size,\n",
    "            out_size=\"scalar\",\n",
    "            width_sizes=hidden_layer_sizes,\n",
    "            weight_init=jax.nn.initializers.normal(stddev=0.1), # We require custom weight initialization because LNNs are very picky\n",
    "            activation=ACTIVATIONS[activation],\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t: Array, y: Array, u: Array | None = None) -> Array:\n",
    "        q, q_t = jnp.split(y, 2, axis=-1)\n",
    "\n",
    "        # Redefine the Lagrangian in terms of the generalized coordinates and velocities.\n",
    "        # This is necessary to differentiate with respect to q and q_t individually.\n",
    "        def _lagrangian(q, q_t):\n",
    "            y = jnp.concat([q, q_t], axis=-1)\n",
    "            return self.lagrangian(y)\n",
    "\n",
    "        q_tt = jax.numpy.linalg.pinv(jax.hessian(_lagrangian, 1)(q, q_t)) @ (\n",
    "                jax.grad(_lagrangian, 0)(q, q_t)\n",
    "                - jax.jacfwd(jax.grad(_lagrangian, 1), 0)(q, q_t) @ q_t\n",
    "            )\n",
    "        return jnp.concat([q_t, q_tt])\n",
    "    \n",
    "\n",
    "# Here we define the loss function. \n",
    "# We will be using only derivative fitting for this task, which \n",
    "# allows us to train the models faster than with trajectory fitting.\n",
    "def derivative_loss(model: ODESolver, data, batch_axis):\n",
    "    \"\"\"Evaluate the models derivative function and compare it to the true derivative.\"\"\"\n",
    "    ys, ys_t = data\n",
    "    ys_t_pred = jax.vmap(model.func, in_axes=0)(None, ys)\n",
    "    return jnp.mean(jnp.square(ys_t_pred - ys_t))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Task A: Comparing HNNs and LNNs <a id='task_1'></a>\n",
    "## 3.1 Task A.1: Training <a id='task_1_1'></a>\n",
    "With a partner (one person does (a) the other (b)) run the code in the cells below:\n",
    "\n",
    "a) Hamiltonian neural network (HNN)\n",
    "\n",
    "b) Lagrangian neural network (HNN)\n",
    "\n",
    "and then plot the prediction(s) (3rd code cell below). Consider the following questions:\n",
    "\n",
    "1) What differences between the models do you observe during training?\n",
    "2) Pay particular attention to *extrapolation* in time by evaluating the models on time series that extend beyond the duration of the training data (e.g., in the evaluation cell set ``t_max_eval`` to 50 or 100). What do you observe? Compare to the results from the neural ODE from the previous task.\n",
    "3) Do the models preserve the energy exactly? Explain why or why not.\n",
    "4) _(optional, difficult)_ Do the models learn the \"true\" Hamiltonian/Lagrangian? How can the differences be explained?\n",
    "\n",
    "<details>\n",
    "<summary>Click here for a hint for 4)</summary>\n",
    "Think about potential transformations of the Hamiltonian/Lagrangian that leave the evolution equations (Hamiltons equations or the Euler-Lagrange equation) unchanged.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian neural network (HNN)\n",
    "hnn_derivative = HamiltonianNN(\n",
    "    hidden_layer_sizes=[32, 32],    # <<< NN size\n",
    "    activation=\"softplus\",          # <<< Activation function \"softplus\", \"relu\" or \"sigmoid\"\n",
    "    key=model_key\n",
    ")\n",
    "hnn = ODESolver(hnn_derivative)\n",
    "\n",
    "hnn, hist = klax.fit(\n",
    "    hnn,\n",
    "    (ys_flat, ys_t_flat),\n",
    "    steps=25_000,\n",
    "    loss_fn=derivative_loss,\n",
    "    key=loader_key,\n",
    ")\n",
    "hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagrangian neural network (LNN)\n",
    "lnn_derivative = LagrangianNN(\n",
    "    hidden_layer_sizes=[32, 32],    # <<< NN size\n",
    "    activation=\"softplus\",          # <<< Activation function \"softplus\", \"relu\" or \"sigmoid\"\n",
    "    key=model_key\n",
    ")\n",
    "lnn = ODESolver(lnn_derivative)\n",
    "\n",
    "lnn, hist = klax.fit(\n",
    "    lnn,\n",
    "    (ys_flat, ys_t_flat),\n",
    "    steps=25_000,\n",
    "    loss_fn=derivative_loss,\n",
    "    optimizer=optax.adam(1e-4),\n",
    "    key=loader_key,\n",
    ")\n",
    "\n",
    "hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model prediction(s)\n",
    "y0_eval = jnp.array([0.7, -0.2, 0.0, 0.0])  # <<< Initial condition for evaluation\n",
    "t_max_eval = 20                             # <<< Length of evaluation trajectory\n",
    "states = \"positions\"                        # <<< Plot only: \"positions\", \"velocities\" or \"all\"\n",
    "ts_eval = jnp.linspace(0, t_max_eval, 1000)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(12, 11))\n",
    "ys_true = true_system(ts_eval, y0_eval)\n",
    "plot_trajectory(ts_eval, ys_true, ax=axes[0], label=\"True\", states=states, **true_kwargs)\n",
    "E_true = jax.vmap(two_mass_oscillator.get_energy)(ys_true)\n",
    "plot_energy(ts_eval, E_true, ax=axes[1], label=\"True Energy\", **true_kwargs)   # type: ignore\n",
    "plot_energy(ts_eval, E_true, ax=axes[2], label=\"True Energy\", **true_kwargs)   # type: ignore\n",
    "lagrangian_true = jax.vmap(two_mass_oscillator.get_lagrangian)(ys_true)\n",
    "plot_energy(ts_eval, lagrangian_true, ax=axes[3], label=\"\\\"True\\\" Lagrangian\", **true_kwargs)   # type: ignore\n",
    "try:\n",
    "    ys_pred_hnn = hnn(ts_eval, y0_eval)\n",
    "    plot_trajectory(ts_eval, ys_pred_hnn, ax=axes[0], states=states, color=\"red\", label=\"HNN\")\n",
    "    E_pred_hnn = jax.vmap(two_mass_oscillator.get_energy)(ys_pred_hnn)\n",
    "    plot_energy(ts_eval, E_pred_hnn, ax=axes[1], color=\"red\", label=\"Energy of prediction (HNN)\")   # type: ignore\n",
    "    hamiltonian_hnn = jax.vmap(hnn.func.hamiltonian)(ys_pred_hnn)\n",
    "    plot_energy(ts_eval, hamiltonian_hnn, ax=axes[2], color=\"red\", label=\"Hamiltonian (HNN)\")   # type: ignore\n",
    "except NameError:\n",
    "    print(\"You have not trained the Hamiltonian NN (hnn). Skipping this model.\")\n",
    "try:\n",
    "    ys_pred_lnn = lnn(ts_eval, y0_eval)\n",
    "    plot_trajectory(ts_eval, ys_pred_lnn, ax=axes[0], states=states, color=\"green\", label=\"LNN\")\n",
    "    E_pred_lnn = jax.vmap(two_mass_oscillator.get_energy)(ys_pred_lnn)\n",
    "    plot_energy(ts_eval, E_pred_lnn, ax=axes[1], color=\"green\", label=\"Energy of prediction (LNN)\")   # type: ignore\n",
    "    lagrangian_lnn = 20*jax.vmap(lnn.func.lagrangian)(ys_pred_lnn)\n",
    "    plot_energy(ts_eval, lagrangian_lnn, ax=axes[3], color=\"green\", label=\"Lagrangian (LNN)\")   # type: ignore\n",
    "except NameError:\n",
    "    print(\"You have not trained the Lagrangian NN (lnn). Skipping this model.\")\n",
    "axes[2].set(title=\"Hamiltonian\")\n",
    "axes[3].set(title=\"Lagrangian\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Task A.2: Different system parameters <a id='task_1_2'></a>\n",
    "In our data generation we used $m_1=m_2=1$. Go to the [data generation code cell](#data_generation) and \n",
    "\n",
    "1) choose **different** masses, e.g., ``... TwoMassOscillator(m1=3.0, m2=0.5, k1=1.0, k2=0.5)``\n",
    "\n",
    "2) (optional) choose positive dissipation coefficients ``... TwoMassOscillator(m1=3.0, m2=0.5, k1=1.0, k2=0.5, c1=0.4, c2=0.1)``\n",
    "\n",
    "Then rerun the model training again.\n",
    "Can the models still fit the data? If not, why not?\n",
    "\n",
    "<details>\n",
    "<summary>Click here for a hint for a)</summary>\n",
    "For the two mass oscillator the canonical momenta are given by p=m*v.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Theory: Energy dissipation <a id='theory_phnn'></a>\n",
    "\n",
    "You might have noticed in the previous task that the Hamiltonian and Lagrangian NNs cannot learn the dissipative system. This is not suprising, as they are constrained to conserve a \"energy-like\" quantity. In practice most interesting systems are dissipative. In this final task we explore the use of port-Hamiltonian neural networks to learn dissipative systems.\n",
    "\n",
    "### 4.1 Port-Hamiltonian neural network (PHNN)\n",
    "In the port-Hamiltonian modeling framework we extend the Hamiltonian evolution equation by introducing an additional matrix: \n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\dot{\\mathbf{y}} = \\big(\\mathbf{J}(\\mathbf{y}) - \\mathbf{R}(\\mathbf{y})\\big)\\frac{\\partial\\mathcal{H}(\\mathbf{y})}{\\partial\\mathbf{y}}\n",
    "\\end{equation}\n",
    "$$\n",
    "Here $\\mathbf{R}$ is the *positive semi-definite* dissipation matrix and $\\mathbf{J}$ is a *skew-symmetric* structure matrix.\n",
    "Both matrices can potentially depend on the systems state $\\mathbf{J}: \\mathbb{R}^n\\mapsto\\mathbb{R}^{n\\times n}$ and $\\mathbf{R}: \\mathbb{R}^n\\mapsto\\mathbb{R}^{n\\times n}$.\n",
    "(We could also add and input matrix $\\mathbf{G}(\\mathbf{y})$ to allow for external inputs $\\mathbf{u}$, e.g., forces that act on the masses. However, since we do not consider a forced system in this tutorial, we will ignore $\\mathbf{G}$ and $\\mathbf{u}$ here)\n",
    "\n",
    "Whereas the HNN exactly conserved the Hamiltonian $\\mathcal{H}$, the pHNN can dissipate the Hamiltonian. By using the properties of skew-symmetric and positive definite matrices we can derive the dissipation inequality:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\dot{\\mathcal{H}}(\\mathbf{y}(t)) = \\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}} \\dot{\\mathbf{y}} = \\underbrace{\\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}}^\\intercal\\mathbf{J}\\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}}}_{=0} - \\underbrace{\\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}}^\\intercal\\mathbf{R}\\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{y}}}_{\\geq0} \\leq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We parameterize $\\mathcal{H},\\,\\mathbf{J}$ and $\\mathbf{R}$ with neural networks and learn them from data.\n",
    "\n",
    "### 4.2 stable Port-Hamiltonian neural network (sPHNN)\n",
    "The port-Hamiltonian framework above guarantees that the Hamiltonian cannot increase along predicted trajectories. However, so far the Hamiltonian is an unconstrained neural network and can get arbitrarily small/negative. Thus we can still have trajectories that blow up or are unstable. \n",
    "\n",
    "We can provide and additional *inductive bias* by constrianing the Hamiltonian. In particular, we can use a **convex neural network** to parameterize $\\mathcal{H}$. Since a convex function can only have one (global) minimum we can guarantee<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) properties like stability and boundedness this way. This also improves the training.\n",
    "\n",
    "<a name=\"cite_note-1\"></a><sup>1</sup>We would need to ensure that the convex function *actually* has a minimum, but in many cases convex neural networks already provide a strong enough bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell implements (s)PHNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortHamiltonianNN(eqx.Module):\n",
    "    hamiltonian: klax.nn.MLP | klax.nn.FICNN\n",
    "    J: klax.nn.ConstantSkewSymmetricMatrix\n",
    "    R: klax.nn.ConstantSPDMatrix\n",
    "\n",
    "    def __init__(self, convex_hamiltonian=False, *, key: PRNGKeyArray):\n",
    "        NN_type = klax.nn.FICNN if convex_hamiltonian else klax.nn.MLP\n",
    "        self.hamiltonian = NN_type(\n",
    "            in_size=4,\n",
    "            out_size=\"scalar\",\n",
    "            width_sizes=[16, 16],\n",
    "            key=key,\n",
    "        )\n",
    "        self.J = klax.nn.ConstantSkewSymmetricMatrix(\n",
    "            shape=(4,4),\n",
    "            key=key,\n",
    "        )\n",
    "        self.R = klax.nn.ConstantSPDMatrix(\n",
    "            shape=(4,4),\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t: Array, y: Array, u: Array | None = None) -> Array:\n",
    "        grad_H = jax.grad(self.hamiltonian)(y)\n",
    "        return (self.J(y) - self.R(y)) @ grad_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task B: (s)PHNNs <a id='task_2'></a>\n",
    "\n",
    "We now consider the damped two mass oscillator system:\n",
    "\n",
    "<img src=\"images\\Two_mass_oscillator_damped.png\" height=\"200\"/>\n",
    "\n",
    "Go to the  [data generation code cell](#twomassdatageneration) and choose positive dissipation coefficients and non-equal masses. For example:  ``... TwoMassOscillator(m1=3.0, m2=0.5, k1=1.0, k2=0.5, c1=0.4, c2=0.1)``\n",
    "\n",
    "Then use the code cells below to train both a PHNN and a sPHNN.\n",
    "\n",
    "Consider the following questions:\n",
    "\n",
    "1) Are there performance differences between the PHNN and the sPHNN? How might they be explained?\n",
    "2) _(optional, difficult)_: Can (s)PHNN learn the system with different masses? Why might it succeed here, where HNNs failed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Port-Hamiltonian neural network (PHNN)\n",
    "phnn_derivative = PortHamiltonianNN(\n",
    "    convex_hamiltonian = False,\n",
    "    key=model_key\n",
    ")\n",
    "phnn = ODESolver(phnn_derivative)\n",
    "\n",
    "phnn, hist = klax.fit(\n",
    "    phnn,\n",
    "    (ys_flat, ys_t_flat),\n",
    "    steps=25_000,\n",
    "    loss_fn=derivative_loss,\n",
    "    key=loader_key,\n",
    ")\n",
    "\n",
    "hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable Port-Hamiltonian neural network (sPHNN)\n",
    "sphnn_derivative = PortHamiltonianNN(\n",
    "    convex_hamiltonian = True,\n",
    "    key=model_key\n",
    ")\n",
    "sphnn = ODESolver(sphnn_derivative)\n",
    "\n",
    "sphnn, hist = klax.fit(\n",
    "    sphnn,\n",
    "    (ys_flat, ys_t_flat),\n",
    "    steps=25_000,\n",
    "    loss_fn=derivative_loss,\n",
    "    key=loader_key,\n",
    ")\n",
    "\n",
    "hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model prediction(s)\n",
    "y0_eval = jnp.array([0.7, -0.8, 0.0, 0.0])  # <<< Initial condition for evaluation\n",
    "t_max_eval = 20                             # <<< Length of evaluation trajectory\n",
    "states = \"positions\"                        # <<< Plot only: \"positions\", \"velocities\" or \"all\"\n",
    "ts_eval = jnp.linspace(0, t_max_eval, 1000)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9))\n",
    "ys_true = true_system(ts_eval, y0_eval)\n",
    "plot_trajectory(ts_eval, ys_true, ax=axes[0], label=\"True State\", states=states, **true_kwargs)\n",
    "E_true = jax.vmap(two_mass_oscillator.get_energy)(ys_true)\n",
    "plot_energy(ts_eval, E_true, ax=axes[1], label=\"True Energy\", **true_kwargs)   # type: ignore\n",
    "plot_energy(ts_eval, E_true, ax=axes[2], label=\"True Hamiltonian\", **true_kwargs)   # type: ignore\n",
    "\n",
    "ys_phnn = klax.finalize(phnn)(ts_eval, y0_eval)\n",
    "plot_trajectory(ts_eval, ys_phnn, ax=axes[0], states=states, color=\"orange\", label=\"PHNN State\")\n",
    "E_phnn = jax.vmap(two_mass_oscillator.get_energy)(ys_phnn)\n",
    "plot_energy(ts_eval, E_phnn, ax=axes[1], color=\"orange\", label=\"Energy of prediction (PHNN)\")   # type: ignore\n",
    "hamiltonian_phnn = jax.vmap(klax.finalize(phnn.func.hamiltonian))(ys_phnn)\n",
    "# hamiltonian_phnn += E_true[0]-hamiltonian_phnn[0]   # type: ignore\n",
    "plot_energy(ts_eval, hamiltonian_phnn, ax=axes[2], color=\"orange\", label=\"Hamiltonian (PHNN)\")   # type: ignore\n",
    "\n",
    "ys_sphnn = klax.finalize(sphnn)(ts_eval, y0_eval)\n",
    "plot_trajectory(ts_eval, ys_sphnn, ax=axes[0], states=states, color=\"green\", label=\"sPHNN State\")\n",
    "E_sphnn = jax.vmap(two_mass_oscillator.get_energy)(ys_sphnn)\n",
    "plot_energy(ts_eval, E_sphnn, ax=axes[1], color=\"green\", label=\"Energy of prediction (sPHNN)\")   # type: ignore\n",
    "hamiltonian_sphnn = jax.vmap(klax.finalize(sphnn.func.hamiltonian))(ys_sphnn)\n",
    "# hamiltonian_sphnn += E_true[0]-hamiltonian_sphnn[0] # type: ignore\n",
    "plot_energy(ts_eval, hamiltonian_sphnn, ax=axes[2], color=\"green\", label=\"Hamiltonian (sPHNN)\")   # type: ignore\n",
    "\n",
    "axes[2].set(title=\"Hamiltonian\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
